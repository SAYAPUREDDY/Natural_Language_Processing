{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "NLP is focused on the interaction between computers and humans through natural language.The ultimate goal of the NLP is to enable computers to understand,interpret,and generate human language in a way that is both meaningful and useful.\n",
    "This involves several tasks :\n",
    "- Text analysis\n",
    "- sentiment analysis\n",
    "- Machine Translation\n",
    "- Speech recognition\n",
    "- Natural language generation\n",
    "- Named entity recognition\n",
    "- Part-of-speech tagging\n",
    "- coreference resolution\n",
    "- Question answering\n",
    "# Approach of Learning\n",
    "1. Text Preprocessing Level 1- Tokenization,Lemmatization,StopWords,POS\n",
    "2. Text Preprocessing Level 2- Bag Of  Words, TFIDF, Unigrams,Bigrams,n-grams\n",
    "3. Text Preprocessing- Gensim,Word2vec,AvgWord2vec\n",
    "4. Solve Machine Learning Usecases\n",
    "5. Get the Understanding Of Artificial Neural Network\n",
    "6. Understanding Recurrent Neural Networks, LSTM,GRU\n",
    "7. Text Preprocessing Level 3- Word Embeddings, Word2vec\n",
    "8. Bidirectional LSTM RNN, Encoders And Decoders, Attention Models\n",
    "9. Transformers \n",
    "10. BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Level 1- Tokenization,Lemmatization,StopWords,POS\n",
    "## 1. Tokenization\n",
    "- converting the paragarphs into sentences.\n",
    "- involves breaking down a stream of text into smaller units called tokens.\n",
    "-  it transforms raw text into a structured format that can be more easily analyzed by NLP models and algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenisation of the paragarph\n",
    "import nltk \n",
    "#nltk.download() this is to download all the packages from the nltk library\n",
    "\n",
    "paragraph = \"\"\"When it comes to speaking a lot of people get nervous about it. A well-written speech makes giving the speech easier. The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech. These are important in any speech. The next step is writing the thesis that will be used throughout writing the speech. Research the topic and get evidence for supporting the thesis. Look for statistics or quotes that will support the thesis. In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When it comes to speaking a lot of people get nervous about it.', 'A well-written speech makes giving the speech easier.', 'The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech.', 'These are important in any speech.', 'The next step is writing the thesis that will be used throughout writing the speech.', 'Research the topic and get evidence for supporting the thesis.', 'Look for statistics or quotes that will support the thesis.', 'In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.']\n",
      "['When', 'it', 'comes', 'to', 'speaking', 'a', 'lot', 'of', 'people', 'get', 'nervous', 'about', 'it', '.', 'A', 'well-written', 'speech', 'makes', 'giving', 'the', 'speech', 'easier', '.', 'The', 'steps', 'in', 'writing', 'a', 'speech', 'begin', 'with', 'choosing', 'the', 'topic', ',', 'recognizing', 'the', 'audience', ',', 'knowing', 'the', 'purpose', 'of', 'the', 'speech', ',', 'and', 'the', 'occasion', 'for', 'the', 'speech', '.', 'These', 'are', 'important', 'in', 'any', 'speech', '.', 'The', 'next', 'step', 'is', 'writing', 'the', 'thesis', 'that', 'will', 'be', 'used', 'throughout', 'writing', 'the', 'speech', '.', 'Research', 'the', 'topic', 'and', 'get', 'evidence', 'for', 'supporting', 'the', 'thesis', '.', 'Look', 'for', 'statistics', 'or', 'quotes', 'that', 'will', 'support', 'the', 'thesis', '.', 'In', 'the', 'writing', 'of', 'a', 'speech', 'it', 'is', 'important', 'to', 'have', 'three', 'main', 'points', 'to', 'support', 'the', 'thesis', 'and', 'a', 'conclusion', 'at', 'the', 'end', 'of', 'the', 'speech', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph = \"\"\"When it comes to speaking a lot of people get nervous about it. A well-written speech makes giving the speech easier. The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech. These are important in any speech. The next step is writing the thesis that will be used throughout writing the speech. Research the topic and get evidence for supporting the thesis. Look for statistics or quotes that will support the thesis. In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.\"\"\"\n",
    "#tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph) \n",
    "print(sentences)\n",
    "\n",
    "#tokeninzing words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    " The process of reducing infected words to thier word stem.\n",
    "- the main difference between lemmatization and stemming is the lemmiatization will provide the meaningful word stem,where as the stemming provides very rarely.\n",
    "- lemmatization takes more time as it has to process lot to provide meaningful words.\n",
    "- stemming application cases are sentiment analysis,gmail-spam classifier\n",
    "- lemmatization used in chatbots,question answer applications as it response is meaningful\n",
    "# 1. Stemming\n",
    "- problem : the produced intermediate representation of the word may not have any meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when come speak lot peopl get nervou .\n",
      "a well-written speech make give speech easier .\n",
      "the step write speech begin choos topic , recogn audienc , know purpos speech , occas speech .\n",
      "these import speech .\n",
      "the next step write thesi use throughout write speech .\n",
      "research topic get evid support thesi .\n",
      "look statist quot support thesi .\n",
      "in write speech import three main point support thesi conclus end speech .\n"
     ]
    }
   ],
   "source": [
    "#the proterstemmer is used for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "#stopwords help us to remove the words that doesnt put much value in the scenario, examole:for,the,of etc..,\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "#Stemming\n",
    "for i in range(len(sentences)):\n",
    "    #taking the sentences and tokinzing them\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    #the next lines stem all the words that are not in the stopwords from the english language \n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    #join all the remainig words and make the sentences without stopwords\n",
    "    sentences[i]=' '.join(words)\n",
    "    print(sentences[i])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Lemmatization\n",
    "This overcomes the above problem of stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When come speaking lot people get nervous .\n",
      "A well-written speech make giving speech easier .\n",
      "The step writing speech begin choosing topic , recognizing audience , knowing purpose speech , occasion speech .\n",
      "These important speech .\n",
      "The next step writing thesis used throughout writing speech .\n",
      "Research topic get evidence supporting thesis .\n",
      "Look statistic quote support thesis .\n",
      "In writing speech important three main point support thesis conclusion end speech .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "#Stemming\n",
    "for i in range(len(sentences)):\n",
    "    #taking the sentences and tokinzing them\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    #the next lines lemmatize all the words that are not in the stopwords from the english language \n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    #join all the remainig words and make the sentences without stopwords\n",
    "    sentences[i]=' '.join(words)\n",
    "    print(sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords\n",
    "- Stop words are basically a set of commonly used words in any language, not just English.\n",
    "- The reason why stop words are critical to many applications is that, if we remove the words that are very commonly used in a given language, we can focus on the important words instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic implementation of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "#to dowmload the stopwords package of all the languages \n",
    "#nltk.download('stopwords')\n",
    "stopwords.words('english')\n",
    "# these are just basic implementation of the stopwords , to use that while steming or any other operation, check the other implementations where we would have used the stopwords while data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech Tags\n",
    "- In corpus linguistics, part-of-speech tagging , also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context.\n",
    "- Types of POS in NLP:\n",
    "- CC coordinating junction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: \"there is\" ... think of it like \"there exists\" )\n",
    "- FW foriegn word\n",
    "- IN preposition/subordinating conjuction\n",
    "- JJ adjective - 'big'\n",
    "- JJr adjective,superlative - 'bigger'\n",
    "- JJS adjective,superlative - 'biggest'\n",
    "- LS list marker 1)\n",
    "- MD modal - could,will\n",
    "- NN noun,singular- \"desk\"\n",
    "- NNS noun plural- \"desks\"\n",
    "- NNP proper noun,singular\n",
    "- NNPS proper noun,plural\n",
    "- PDT predeterminer\n",
    "- POS possesive ending parents\n",
    "- PRP personal pronoun \n",
    "- PRP$ possesive pronoun\n",
    "- RB adverb\n",
    "- RBR adverb,comparitive-better\n",
    "- RBS Adverb,superlative-best\n",
    "- RP particle\n",
    "- TO - to go\n",
    "- UH interjection\n",
    "- VB verb,baseform\n",
    "- VBD verb,past tense\n",
    "- VBG verb,present\n",
    "- VBN verb,past participle\n",
    "- VBP verb, sing.present,non-3d-take\n",
    "- VBZ verb,3rd person sing.present-takes\n",
    "- WDT wh-determiner-which\n",
    "- WP wh-pronoun -who,what\n",
    "- WP$ possesive wh-pronoun, eg-whose\n",
    "- WRB wh-adverb,eg-where,when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('live', 'VBP'), ('in', 'IN'), ('Germany', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# applying on single sentence\n",
    "line=\"I live in Germany\"\n",
    "line=line.split()\n",
    "print(nltk.pos_tag(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('When', 'WRB'), ('it', 'PRP'), ('comes', 'VBZ'), ('to', 'TO'), ('speaking', 'VBG'), ('a', 'DT'), ('lot', 'NN'), ('of', 'IN'), ('people', 'NNS'), ('get', 'VBP'), ('nervous', 'JJ'), ('about', 'IN'), ('it', 'PRP'), ('.', '.')]\n",
      "[('A', 'DT'), ('well-written', 'JJ'), ('speech', 'NN'), ('makes', 'VBZ'), ('giving', 'VBG'), ('the', 'DT'), ('speech', 'NN'), ('easier', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('steps', 'NNS'), ('in', 'IN'), ('writing', 'VBG'), ('a', 'DT'), ('speech', 'NN'), ('begin', 'NN'), ('with', 'IN'), ('choosing', 'VBG'), ('the', 'DT'), ('topic', 'NN'), (',', ','), ('recognizing', 'VBG'), ('the', 'DT'), ('audience', 'NN'), (',', ','), ('knowing', 'VBG'), ('the', 'DT'), ('purpose', 'NN'), ('of', 'IN'), ('the', 'DT'), ('speech', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('occasion', 'NN'), ('for', 'IN'), ('the', 'DT'), ('speech', 'NN'), ('.', '.')]\n",
      "[('These', 'DT'), ('are', 'VBP'), ('important', 'JJ'), ('in', 'IN'), ('any', 'DT'), ('speech', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('next', 'JJ'), ('step', 'NN'), ('is', 'VBZ'), ('writing', 'VBG'), ('the', 'DT'), ('thesis', 'NN'), ('that', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('throughout', 'IN'), ('writing', 'VBG'), ('the', 'DT'), ('speech', 'NN'), ('.', '.')]\n",
      "[('Research', 'NN'), ('the', 'DT'), ('topic', 'NN'), ('and', 'CC'), ('get', 'VB'), ('evidence', 'NN'), ('for', 'IN'), ('supporting', 'VBG'), ('the', 'DT'), ('thesis', 'NN'), ('.', '.')]\n",
      "[('Look', 'NN'), ('for', 'IN'), ('statistics', 'NNS'), ('or', 'CC'), ('quotes', 'VBZ'), ('that', 'WDT'), ('will', 'MD'), ('support', 'VB'), ('the', 'DT'), ('thesis', 'NN'), ('.', '.')]\n",
      "[('In', 'IN'), ('the', 'DT'), ('writing', 'NN'), ('of', 'IN'), ('a', 'DT'), ('speech', 'NN'), ('it', 'PRP'), ('is', 'VBZ'), ('important', 'JJ'), ('to', 'TO'), ('have', 'VB'), ('three', 'CD'), ('main', 'JJ'), ('points', 'NNS'), ('to', 'TO'), ('support', 'VB'), ('the', 'DT'), ('thesis', 'NN'), ('and', 'CC'), ('a', 'DT'), ('conclusion', 'NN'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('the', 'DT'), ('speech', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# we will apply the POS tagging\n",
    "sen=nltk.sent_tokenize(paragraph)\n",
    "#print(sen)\n",
    "for i in range(len(sen)):\n",
    "    words=nltk.word_tokenize(sen[i])\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity Recognition\n",
    "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
    "words=nltk.word_tokenize(sentence)\n",
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1280.0,168.0\" width=\"1280px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"3.125%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">The</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.5625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"9.375%\" x=\"3.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Tower</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"7.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"12.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.0625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"15.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">built</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"17.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"20%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">from</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"23.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1887</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25.625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"27.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"28.75%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.75%\" x=\"30%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">1889</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.875%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"33.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.625%\" x=\"36.25%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"52.9412%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Gustave</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.4706%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"47.0588%\" x=\"52.9412%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Eiffel</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.4706%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"41.5625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"46.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"47.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"48.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">whose</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">WP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.9375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"5.625%\" x=\"53.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.9375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"8.125%\" x=\"58.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">specialized</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"66.875%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.25%\" x=\"69.375%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">building</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.5%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.375%\" x=\"75.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">metal</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.8125%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">frameworks</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.75%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.125%\" x=\"87.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.0625%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.5%\" x=\"90.625%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">structures</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.375%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"1.875%\" x=\"98.125%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.0625%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('The', 'DT'), Tree('ORGANIZATION', [('Eiffel', 'NNP'), ('Tower', 'NNP')]), ('was', 'VBD'), ('built', 'VBN'), ('from', 'IN'), ('1887', 'CD'), ('to', 'TO'), ('1889', 'CD'), ('by', 'IN'), Tree('PERSON', [('Gustave', 'NNP'), ('Eiffel', 'NNP')]), (',', ','), ('whose', 'WP$'), ('company', 'NN'), ('specialized', 'VBD'), ('in', 'IN'), ('building', 'NN'), ('metal', 'NN'), ('frameworks', 'NNS'), ('and', 'CC'), ('structures', 'NNS'), ('.', '.')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "nltk.ne_chunk(tag_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Level 2- Bag Of  Words, TFIDF, Unigrams,Bigrams,n-grams\n",
    "# 1. Bag of words\n",
    "- The bag of words (BoW) model is a simplified representation used in natural language processing and information retrieval where a text is represented as an unordered collection (bag) of its words, disregarding grammar and word order but maintaining the multiplicity of the words.\n",
    "- the text is represented as a frequency vector, where each element of the vector corresponds to a word in the vocabulary of the corpus, and the value indicates the number of times that word appears in the text.\n",
    "\n",
    "\n",
    "- Disadvantage: the features extracted will be having equal weightage sometimes,we cant decide which word is more important.\n",
    "- not a good fit for huge dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "paragraph=\"\"\"When it comes to speaking a lot of people get nervous about it. A well-written speech makes giving the speech easier. The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech. These are important in any speech. The next step is writing the thesis that will be used throughout writing the speech. Research the topic and get evidence for supporting the thesis. Look for statistics or quotes that will support the thesis. In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the texts\n",
    "import re  # for regular expression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come speak lot peopl get nervou\n",
      "well written speech make give speech easier\n",
      "step write speech begin choos topic recogn audienc know purpos speech occas speech\n",
      "import speech\n",
      "next step write thesi use throughout write speech\n",
      "research topic get evid support thesi\n",
      "look statist quot support thesi\n",
      "write speech import three main point support thesi conclus end speech\n"
     ]
    }
   ],
   "source": [
    "#Using the stemming\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "#iterating through all the sentences, by cleaning all the commas,full stops and question marks\n",
    "for i in range(len(sentences)):\n",
    "    #apart from a-z and A-Z it will remove everything with spaces\n",
    "    review=re.sub('[^a-zA-Z]', ' ',sentences[i])\n",
    "    #lowering words of each and every sentences\n",
    "    review=review.lower()\n",
    "    # splits the sentneces and provide the list of words\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in set (stopwords.words('english'))]\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come speaking lot people get nervous\n",
      "well written speech make giving speech easier\n",
      "step writing speech begin choosing topic recognizing audience knowing purpose speech occasion speech\n",
      "important speech\n",
      "next step writing thesis used throughout writing speech\n",
      "research topic get evidence supporting thesis\n",
      "look statistic quote support thesis\n",
      "writing speech important three main point support thesis conclusion end speech\n"
     ]
    }
   ],
   "source": [
    "#using lemmatization\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "#iterating through all the sentences, by cleaning all the commas,full stops and question marks\n",
    "for i in range(len(sentences)):\n",
    "    #apart from a-z and A-Z it will remove everything with spaces\n",
    "    review=re.sub('[^a-zA-Z]', ' ',sentences[i])\n",
    "    #lowering words of each and every sentences\n",
    "    review=review.lower()\n",
    "    # splits the sentneces and provide the list of words\n",
    "    review=review.split()\n",
    "    review=[wordnet.lemmatize(word) for word in review if not word in set (stopwords.words('english'))]\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   audience  begin  choosing  come  conclusion  easier  end  evidence  get  \\\n",
      "0         0      0         0     1           0       0    0         0    1   \n",
      "1         0      0         0     0           0       1    0         0    0   \n",
      "2         1      1         1     0           0       0    0         0    0   \n",
      "3         0      0         0     0           0       0    0         0    0   \n",
      "4         0      0         0     0           0       0    0         0    0   \n",
      "5         0      0         0     0           0       0    0         1    1   \n",
      "6         0      0         0     0           0       0    0         0    0   \n",
      "7         0      0         0     0           1       0    1         0    0   \n",
      "\n",
      "   giving  ...  support  supporting  thesis  three  throughout  topic  used  \\\n",
      "0       0  ...        0           0       0      0           0      0     0   \n",
      "1       1  ...        0           0       0      0           0      0     0   \n",
      "2       0  ...        0           0       0      0           0      1     0   \n",
      "3       0  ...        0           0       0      0           0      0     0   \n",
      "4       0  ...        0           0       1      0           1      0     1   \n",
      "5       0  ...        0           1       1      0           0      1     0   \n",
      "6       0  ...        1           0       1      0           0      0     0   \n",
      "7       0  ...        1           0       1      1           0      0     0   \n",
      "\n",
      "   well  writing  written  \n",
      "0     0        0        0  \n",
      "1     1        0        1  \n",
      "2     0        1        0  \n",
      "3     0        0        0  \n",
      "4     0        2        0  \n",
      "5     0        0        0  \n",
      "6     0        0        0  \n",
      "7     0        1        0  \n",
      "\n",
      "[8 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# creating bag of words \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "# Transform the paragraph into a bag of words\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "# Get the feature names (words)\n",
    "feature_names = cv.get_feature_names_out()\n",
    "# Create a DataFrame for better visualization\n",
    "df = pd.DataFrame(x, columns=feature_names)\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TF-IDF (Term Frequency and Inverse Document Frequency)\n",
    "- It is a widely used statistical method in natural language processing and information retrieval. It measures how important a term is within a document relative to a collection of documents (i.e., relative to a corpus).\n",
    "- Words within a text document are transformed into importance numbers by a text vectorization process.\n",
    "# 1.Term Frequency : \n",
    "TF of a word is the number of times the term appears in the document compared to the total number of words in the Document.\n",
    "$$\n",
    "\\text{TF} = \\frac{\\text{number of repetitions of the word in the sentence}}{\\text{number of words in the sentence}}\n",
    "$$\n",
    "# 2.Inverse Document Frequency\n",
    "$$\n",
    "\\text{IDF} = \\log \\left( \\frac{\\text{total number of documents}}{\\text{number of documents containing the word}} \\right)\n",
    "$$\n",
    "\n",
    "The TF-IDF term is calculated by multiplying TF and IDF scores by using the above formulas\n",
    "$$\n",
    "\\text{TF-IDF}=\\text{TF} * \\text{IDF}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk \n",
    "import pandas as pd\n",
    "paragraph = \"\"\"When it comes to speaking a lot of people get nervous about it. A well-written speech makes giving the speech easier. The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech. These are important in any speech. The next step is writing the thesis that will be used throughout writing the speech. Research the topic and get evidence for supporting the thesis. Look for statistics or quotes that will support the thesis. In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.\"\"\"\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]', ' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wordnet.lemmatize(word) for word in review if not word in set (stopwords.words('english'))]\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    #print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   audience     begin  choosing      come  conclusion    easier       end  \\\n",
      "0  0.000000  0.000000  0.000000  0.418767    0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000    0.000000  0.399677  0.000000   \n",
      "2  0.291569  0.291569  0.291569  0.000000    0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "6  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   \n",
      "7  0.000000  0.000000  0.000000  0.000000    0.341197  0.000000  0.341197   \n",
      "\n",
      "   evidence       get    giving  ...   support  supporting    thesis  \\\n",
      "0  0.000000  0.350959  0.000000  ...  0.000000    0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.399677  ...  0.000000    0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000    0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000000    0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000    0.000000  0.248487   \n",
      "5  0.456112  0.382258  0.000000  ...  0.000000    0.456112  0.289212   \n",
      "6  0.000000  0.000000  0.000000  ...  0.413674    0.000000  0.312981   \n",
      "7  0.000000  0.000000  0.000000  ...  0.285950    0.000000  0.216347   \n",
      "\n",
      "      three  throughout     topic      used      well   writing   written  \n",
      "0  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000    0.000000  0.000000  0.000000  0.399677  0.000000  0.399677  \n",
      "2  0.000000    0.000000  0.244358  0.000000  0.000000  0.210861  0.000000  \n",
      "3  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.000000    0.391886  0.000000  0.391886  0.000000  0.566818  0.000000  \n",
      "5  0.000000    0.000000  0.382258  0.000000  0.000000  0.000000  0.000000  \n",
      "6  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "7  0.341197    0.000000  0.000000  0.000000  0.000000  0.246751  0.000000  \n",
      "\n",
      "[8 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "#creating TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "feature_names = cv.get_feature_names_out()\n",
    "df = pd.DataFrame(x, columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words and TF-IDF Problems\n",
    "- Both BOW and TF-IDF approach semantic information is not stored\n",
    "- TF-IDF gives importance to uncommon words.\n",
    "- There is definitely chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing- Gensim,Word2vec,AvgWord2vec\n",
    "# 1. Word2vec\n",
    "- In this specific model,each word is basically represented as a vector of 32 or more dimension instead of a single number.\n",
    "- Here the semantic information and relation between different words is also preserved.\n",
    "\n",
    "- steps to create  Word2vec :\n",
    "1. Tokenization of sentences\n",
    "2. Create Histograms\n",
    "3. Take most frequent words\n",
    "4. Create a matrix with all unique words.It also represent the occurrence relation between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "paragraph = \"\"\"When it comes to speaking a lot of people get nervous about it. A well-written speech makes giving the speech easier. The steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech. These are important in any speech. The next step is writing the thesis that will be used throughout writing the speech. Research the topic and get evidence for supporting the thesis. Look for statistics or quotes that will support the thesis. In the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data\n",
    "text=re.sub(r'[[0-9]*\\]',' ',paragraph)\n",
    "text=re.sub(r'\\s+',' ',text)\n",
    "text=text.lower()\n",
    "text=re.sub(r'\\d',' ',text)\n",
    "text=re.sub(r'\\s+',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when it comes to speaking a lot of people get nervous about it.', 'a well-written speech makes giving the speech easier.', 'the steps in writing a speech begin with choosing the topic, recognizing the audience, knowing the purpose of the speech, and the occasion for the speech.', 'these are important in any speech.', 'the next step is writing the thesis that will be used throughout writing the speech.', 'research the topic and get evidence for supporting the thesis.', 'look for statistics or quotes that will support the thesis.', 'in the writing of a speech it is important to have three main points to support the thesis and a conclusion at the end of the speech.']\n"
     ]
    }
   ],
   "source": [
    "#preparing the dataset\n",
    "sentences=nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comes', 'speaking', 'lot', 'people', 'get', 'nervous', '.']\n",
      "['well-written', 'speech', 'makes', 'giving', 'speech', 'easier', '.']\n",
      "['steps', 'writing', 'speech', 'begin', 'choosing', 'topic', ',', 'recognizing', 'audience', ',', 'knowing', 'purpose', 'speech', ',', 'occasion', 'speech', '.']\n",
      "['important', 'speech', '.']\n",
      "['next', 'step', 'writing', 'thesis', 'used', 'throughout', 'writing', 'speech', '.']\n",
      "['research', 'topic', 'get', 'evidence', 'supporting', 'thesis', '.']\n",
      "['look', 'statistics', 'quotes', 'support', 'thesis', '.']\n",
      "['writing', 'speech', 'important', 'three', 'main', 'points', 'support', 'thesis', 'conclusion', 'end', 'speech', '.']\n"
     ]
    }
   ],
   "source": [
    "#now we convert those sentences into words and later remove the unnecassry words\n",
    "sentences=[nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i]=[word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.3722790e-04  2.3260158e-04  5.1003690e-03  9.0100123e-03\n",
      " -9.3066990e-03 -7.1236370e-03  6.4811334e-03  8.9840451e-03\n",
      " -5.0300867e-03 -3.7715547e-03  7.3711746e-03 -1.5406111e-03\n",
      " -4.5317975e-03  6.5671504e-03 -4.8678499e-03 -1.8211382e-03\n",
      "  2.8962421e-03  9.8016835e-04 -8.2873870e-03 -9.4734570e-03\n",
      "  7.3234709e-03  5.0722933e-03  6.7760758e-03  7.5178384e-04\n",
      "  6.3463179e-03 -3.4109638e-03 -9.6431881e-04  5.7709105e-03\n",
      " -7.5247120e-03 -3.9253565e-03 -7.4843387e-03 -9.3432225e-04\n",
      "  9.5359618e-03 -7.3412657e-03 -2.3395098e-03 -1.9245702e-03\n",
      "  8.0836248e-03 -5.9224688e-03  4.8915546e-05 -4.7659352e-03\n",
      " -9.6056685e-03  5.0013419e-03 -8.7618483e-03 -4.4030230e-03\n",
      " -3.6073532e-05 -3.0963137e-04 -7.6732612e-03  9.6120872e-03\n",
      "  5.0001456e-03  9.2454925e-03 -8.1553711e-03  4.4867094e-03\n",
      " -4.1453922e-03  8.3016057e-04  8.5066715e-03 -4.4552432e-03\n",
      "  4.5353863e-03 -6.7849569e-03 -3.5581249e-03  9.3930494e-03\n",
      " -1.5847937e-03  3.0607521e-04 -4.1284421e-03 -7.6865135e-03\n",
      " -1.5213068e-03  2.4870257e-03 -8.9437171e-04  5.5261999e-03\n",
      " -2.7576210e-03  2.2631008e-03  5.4583615e-03  8.3516818e-03\n",
      " -1.4377644e-03 -9.2058321e-03  4.3785479e-03  5.5227318e-04\n",
      "  7.4409833e-03 -8.0346409e-04 -2.6488730e-03 -8.7427013e-03\n",
      " -8.6952641e-04  2.8285771e-03  5.3960774e-03  7.0710024e-03\n",
      " -5.7007498e-03  1.8414281e-03  6.0889134e-03 -4.7827470e-03\n",
      " -3.1066877e-03  6.8053999e-03  1.6493968e-03  1.9857931e-04\n",
      "  3.4813832e-03  2.0254400e-04  9.6326591e-03  5.0674635e-03\n",
      " -8.9128995e-03 -7.0468183e-03  9.0186868e-04  6.3949851e-03]\n",
      "[('well-written', 0.21898989379405975), ('easier', 0.21637500822544098), ('quotes', 0.1958557665348053), ('look', 0.15192197263240814), ('throughout', 0.14197053015232086), ('evidence', 0.10929341614246368), ('three', 0.10080210864543915), ('used', 0.09681220352649689), ('steps', 0.09347456693649292), ('get', 0.09287330508232117)]\n"
     ]
    }
   ],
   "source": [
    "#Training the word2vec model\n",
    "#mincount is min number of words \n",
    "model= Word2Vec(sentences=sentences,min_count=1)\n",
    "#print(type(model))\n",
    "#find the vocabulary in the model\n",
    "words = list(model.wv.index_to_key)\n",
    "#using this we can get the vector of the particular word\n",
    "vector=model.wv[\"speech\"]\n",
    "print(vector)\n",
    "#now to find most similiar words \n",
    "similiar_words=model.wv.most_similar('speech')\n",
    "print(similiar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Level 3- Word Embeddings, Word2vec\n",
    "# Word Embedding : \n",
    "- In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations.\n",
    "- The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. \n",
    "- This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences\n",
    "sentences=['this is permanent marker','a cup of tea','you live young','store in this position','you are a good boy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Providing the required parameters to perform one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5514, 6233, 2174, 2698], [4994, 8735, 6540, 2044], [9219, 2058, 5357], [130, 1283, 5514, 1900], [9219, 6776, 4994, 4697, 9756]]\n"
     ]
    }
   ],
   "source": [
    "# ONE Hot Representation\n",
    "onehot_repr=[one_hot(words,voc_size)for words in sentences]\n",
    "print(onehot_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0 5514 6233 2174 2698]\n",
      " [   0    0    0    0 4994 8735 6540 2044]\n",
      " [   0    0    0    0    0 9219 2058 5357]\n",
      " [   0    0    0    0  130 1283 5514 1900]\n",
      " [   0    0    0 9219 6776 4994 4697 9756]]\n"
     ]
    }
   ],
   "source": [
    "sentence_length=8\n",
    "# makes the sentence according to the sentnece length \n",
    "# the paramater padding = pre adds zeros in the first to match the sentence length\n",
    "# if we add post it adds the zeros at the end \n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sentence_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passing the indexes to Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the feature length\n",
    "dim=10\n",
    "# initializing the sequential model and adding the embedded layer\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,10,input_length=sentence_length))\n",
    "# considered the adam optimizer and performance metric as MSE\n",
    "model.compile('adam','mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 8, 10)             100000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 100000 (390.62 KB)\n",
      "Trainable params: 100000 (390.62 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction \n",
    "- we can see how the words are converted into featurized vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 215ms/step\n",
      "[[[-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [ 1.5636053e-02  5.2123070e-03 -1.4793385e-02 -3.1096911e-02\n",
      "    3.4079853e-02  3.3748854e-02 -4.5857109e-02 -2.6328517e-02\n",
      "   -3.6012568e-02  1.2989987e-02]\n",
      "  [ 2.8023485e-02  1.8512417e-02 -3.6805846e-02  3.5892699e-02\n",
      "   -1.0007061e-02 -1.5065782e-03  2.5885489e-02  4.8677716e-02\n",
      "   -2.8342677e-02 -4.2258155e-02]\n",
      "  [ 3.6958549e-02  4.1310456e-02  1.9163873e-02 -3.9642490e-02\n",
      "    4.4186901e-02  3.8545821e-02  2.2558346e-03 -2.3684157e-02\n",
      "    2.0796750e-02  3.2669846e-02]\n",
      "  [ 2.1267068e-02 -4.7922101e-02 -4.3742921e-02  5.6882128e-03\n",
      "    8.9191794e-03  1.7729487e-02 -2.6451696e-02 -3.0972088e-02\n",
      "   -2.3853613e-02 -4.7929026e-02]]\n",
      "\n",
      " [[-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-2.9629398e-02 -4.8990991e-02 -1.0432579e-02 -2.0717299e-02\n",
      "    2.6153076e-02  3.7981048e-03  1.8835340e-02  3.3957932e-02\n",
      "    4.4988282e-03  3.1384002e-02]\n",
      "  [-9.2503652e-03 -2.1843230e-02 -3.6100734e-02 -2.7695943e-02\n",
      "    2.9776882e-02  2.5172260e-02  2.2969570e-02 -3.2962397e-02\n",
      "   -1.3765205e-02  6.1868317e-03]\n",
      "  [-3.1755701e-02 -2.2884918e-02 -1.4292311e-02  3.0350462e-03\n",
      "    2.3627076e-02 -1.8696941e-02  2.5011029e-02 -4.1015960e-02\n",
      "   -4.5639005e-02 -2.6467502e-02]\n",
      "  [ 4.9300287e-02  2.6046041e-02  3.1082656e-02 -2.3182606e-02\n",
      "   -4.3743409e-02  4.6813559e-02  3.2462839e-02 -3.3512868e-02\n",
      "    4.2146575e-02 -2.8372467e-02]]\n",
      "\n",
      " [[-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [ 2.3864295e-02 -2.3171676e-02  2.7845535e-02 -2.6650120e-02\n",
      "   -4.6741236e-02 -2.8245974e-02 -9.0678707e-03 -9.6580386e-03\n",
      "   -1.8004347e-02  3.2884512e-02]\n",
      "  [-2.2264147e-02  2.3277629e-02 -2.0544445e-02  2.0726476e-02\n",
      "   -6.0879067e-04 -6.8350881e-04 -1.9234980e-02  4.8309896e-02\n",
      "    4.0585805e-02 -3.4285951e-02]\n",
      "  [-2.8464902e-02  3.7991907e-02 -1.8531252e-02 -1.8906785e-02\n",
      "   -4.6078898e-02 -1.3282310e-02 -1.6111113e-02  4.2400528e-02\n",
      "   -1.1611022e-02 -3.5425961e-02]]\n",
      "\n",
      " [[-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [ 2.5797043e-02 -4.9066436e-02 -4.9406923e-02  4.6856832e-02\n",
      "   -7.2862878e-03  1.8448960e-02 -3.5438679e-02 -3.2869257e-02\n",
      "    1.9552235e-02 -1.9565964e-02]\n",
      "  [ 2.5662784e-02  2.8749075e-02  3.4357794e-03 -4.5629121e-02\n",
      "   -6.0461462e-05 -8.2381368e-03  1.3861384e-02  3.9411094e-02\n",
      "    1.9846823e-02  2.6092481e-02]\n",
      "  [ 1.5636053e-02  5.2123070e-03 -1.4793385e-02 -3.1096911e-02\n",
      "    3.4079853e-02  3.3748854e-02 -4.5857109e-02 -2.6328517e-02\n",
      "   -3.6012568e-02  1.2989987e-02]\n",
      "  [ 4.8883226e-02 -1.8012833e-02 -4.9044408e-02  3.8940858e-02\n",
      "    5.0549284e-03 -4.3150809e-02  3.5183918e-02 -3.0077625e-02\n",
      "    2.2692457e-03 -4.6032477e-02]]\n",
      "\n",
      " [[-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [-1.8251911e-03 -2.2801531e-02 -1.7691743e-02 -4.9948405e-02\n",
      "   -3.8463105e-02 -4.6133578e-02 -2.0115687e-02  1.8304098e-02\n",
      "    4.3020036e-02  4.8049580e-02]\n",
      "  [ 2.3864295e-02 -2.3171676e-02  2.7845535e-02 -2.6650120e-02\n",
      "   -4.6741236e-02 -2.8245974e-02 -9.0678707e-03 -9.6580386e-03\n",
      "   -1.8004347e-02  3.2884512e-02]\n",
      "  [ 4.6148088e-02  4.9303923e-02  1.2278307e-02 -3.5551287e-02\n",
      "   -8.7161772e-03 -2.1097446e-02  6.7750588e-03  4.2655159e-02\n",
      "   -2.0750320e-02  3.2459226e-02]\n",
      "  [-2.9629398e-02 -4.8990991e-02 -1.0432579e-02 -2.0717299e-02\n",
      "    2.6153076e-02  3.7981048e-03  1.8835340e-02  3.3957932e-02\n",
      "    4.4988282e-03  3.1384002e-02]\n",
      "  [ 4.0052984e-02  5.5146813e-03 -1.5649535e-02  4.9149279e-02\n",
      "    4.3310251e-02  2.7422320e-02  3.7353147e-02 -3.4889102e-02\n",
      "   -1.1258088e-02 -3.2297455e-02]\n",
      "  [-6.1921962e-03 -2.1470821e-02 -3.8428079e-02  2.4550665e-02\n",
      "    4.3971870e-02  1.4713895e-02  1.9030083e-02 -9.0362877e-04\n",
      "   -1.1812996e-02 -2.1957433e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0, 5514, 6233, 2174, 2698])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[-0.00182519 -0.02280153 -0.01769174 -0.04994841 -0.0384631  -0.04613358\n",
      "  -0.02011569  0.0183041   0.04302004  0.04804958]\n",
      " [-0.00182519 -0.02280153 -0.01769174 -0.04994841 -0.0384631  -0.04613358\n",
      "  -0.02011569  0.0183041   0.04302004  0.04804958]\n",
      " [-0.00182519 -0.02280153 -0.01769174 -0.04994841 -0.0384631  -0.04613358\n",
      "  -0.02011569  0.0183041   0.04302004  0.04804958]\n",
      " [-0.00182519 -0.02280153 -0.01769174 -0.04994841 -0.0384631  -0.04613358\n",
      "  -0.02011569  0.0183041   0.04302004  0.04804958]\n",
      " [ 0.01563605  0.00521231 -0.01479338 -0.03109691  0.03407985  0.03374885\n",
      "  -0.04585711 -0.02632852 -0.03601257  0.01298999]\n",
      " [ 0.02802349  0.01851242 -0.03680585  0.0358927  -0.01000706 -0.00150658\n",
      "   0.02588549  0.04867772 -0.02834268 -0.04225815]\n",
      " [ 0.03695855  0.04131046  0.01916387 -0.03964249  0.0441869   0.03854582\n",
      "   0.00225583 -0.02368416  0.02079675  0.03266985]\n",
      " [ 0.02126707 -0.0479221  -0.04374292  0.00568821  0.00891918  0.01772949\n",
      "  -0.0264517  -0.03097209 -0.02385361 -0.04792903]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_docs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM RNN, Encoders And Decoders, Attention Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN\n",
    "- It onnect two hidden layers of opposite directions to the same output.\n",
    "- With this form of generative deep learning, the output layer can get information from past and future states simultaneously.\n",
    "- Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. where as in BRNN the future input information is rechable from thr current state.\n",
    "\n",
    "# Architecture\n",
    "- The principle of BRNN is to split the neurons of a regular RNN into two directions, one for positive time direction , and another for negative time direction.\n",
    "- By using two time directions, input information from the past and future of the current time frame can be used unlike standard RNN which requires the delays for including future information.\n",
    "\n",
    "# Training\n",
    "-  For forward pass, forward states and backward states are passed first, then output neurons are passed. \n",
    "-  For backward pass, output neurons are passed first, then forward states and backward states are passed next.\n",
    "- After forward and backward passes are done, the weights are updated.\n",
    "\n",
    "# Applications\n",
    "- speech recognition(combined with LSTM)\n",
    "- Translation\n",
    "- Handwritten Recognition\n",
    "- Protien structure prediction\n",
    "- Part-of-speech tagging\n",
    "- Dependency parsing\n",
    "- Entity Extraction\n",
    "\n",
    "# Disadvantages\n",
    "- slower compared to the standard RNN \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to sequence with nueral networks\n",
    "- A straightforward application of the Long Short-Term Memory (LSTM) architecture can solve general sequence to sequence problems.\n",
    "- The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation.\n",
    "- then to use another LSTM to extract the output sequence from that vector . The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.\n",
    "- Thus, the goal of the LSTM is to estimate the conditional probability\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
